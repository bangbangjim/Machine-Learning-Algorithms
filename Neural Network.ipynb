{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In here I am going to attempt to build a Neural_Network class from scratch and test it on both simple multi-class classification problem.\n",
    "Neural network works by \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pylab.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from scipy.stats import linregress\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "class Neural_network():\n",
    "    def __init__(self, X = np.array, y = np.array, hidden_layer_size  = tuple, alpha = float, num_iter = int, lamb = float):\n",
    "        self.X = X\n",
    "        # each element in y needs to be converted into the representative vector to allow computation\n",
    "        self.y_class = y\n",
    "        self.unique_class = np.unique(y).tolist()\n",
    "        #dataset is a numpy array with both the X and y (class representation) axis.\n",
    "        self.dataset = np.concatenate((self.X, self.y_class.reshape(-1,1)), axis = 1)\n",
    "        # convert the label into vector representation\n",
    "        self.y = self.label_convert(y)            \n",
    "        self.alpha = alpha\n",
    "        self.num_iter = num_iter\n",
    "        self.num_layer = len(hidden_layer_size) + 2\n",
    "        # initialise the weight matrix here\n",
    "        # the weight matrix of each layer is stored as elements inside a list\n",
    "        # there should be (len(whole_network_size) - 1) number of weight matx\n",
    "        self.weights = []\n",
    "\n",
    "        self.number_of_output = np.unique(y).size\n",
    "        self.whole_network_size = (X.shape[1],) + hidden_layer_size + (self.number_of_output,)\n",
    "        for i in range(len(self.whole_network_size) - 1):\n",
    "            ### initial weight shouldn't be zero\n",
    "            d = self.whole_network_size[i] + 1 # +1 to account for bias unit\n",
    "            limit = np.sqrt(1/d)\n",
    "            # the weight is a N x M matrix, where N is number of unit in the next layer, and M is the number of unit of \\ \n",
    "            # current layer + 1 bias unit\n",
    "            weight = np.random.uniform(-limit, limit, (self.whole_network_size[i+1], self.whole_network_size[i] + 1))\n",
    " #           weight = np.zeros()\n",
    "            self.weights.append(weight)\n",
    "        self.lamb = lamb\n",
    "        self.M = self.X.shape[0]\n",
    "        \n",
    "    def sigmoid(self, z = np.array):\n",
    "        h = 1/(1+np.exp(-z))\n",
    "        return h\n",
    "    def softmax(self, z = np.array):\n",
    "        #compute softmax of z (z is an N x M array, where N is the number of features and M is the number of training examples)\n",
    "        h = np.exp(z)/np.sum(np.exp(z), axis = 0)\n",
    "        return h\n",
    "    def forward_propagation(self, x = np.array):\n",
    "        # this is perform on one training example each time, and return the output as vector\n",
    "        # for now only use sigmoid function for activation hiddent and input layer\n",
    "        # forward propagation compute the sigmoid output of the dot product of Wi and ai up to the second last layer.\n",
    "        # the output of that would be the next layer input \n",
    "        # current_input is a N x M matrix, where N is the number of features and M is the number of examples\n",
    "        # the input (a) of each layer is stored in a list as vectors and has the structure of input layer + hidden layer\n",
    "\n",
    "        inputs = []\n",
    " #       print (x)\n",
    "        # start off with the features (X) as the first layer input (with bias unit). Input can always be expressed as a vector\n",
    "        current_input = np.concatenate((np.ones((x.shape[0], 1)), x), axis = 1).T.astype(np.float64)\n",
    "  #      current_input = np.insert(x, 0, 1.0).astype(np.float64).reshape(-1,1)\n",
    "        if current_input.shape != (x.shape[1]+1,x.shape[0]):\n",
    "            logging.warning(\"initial current_input shape is \" + str(current_input.shape) + \" when it is supposed to be \" + str((x.shape[1]+1,x.shape[0])))\n",
    "            \n",
    "  #      current_input = np.concatenate(np.ones(x.shape), x).T\n",
    "        inputs.append(current_input)\n",
    "        for i in range(len(self.whole_network_size) -2):\n",
    "            z = np.dot(self.weights[i], current_input)\n",
    "            if z.shape != (self.whole_network_size[i+1], x.shape[0]):\n",
    "                logging.warning(\"z shape is \" + str(z.shape) + \" when it is supposed to be \" + str((self.whole_network_size[i+1], x.shape[0])))\n",
    "            h = self.sigmoid(z)\n",
    "            #add a bias unit to each training example\n",
    "            current_input = np.concatenate((np.ones((1, x.shape[0])), h), axis = 0).astype(np.float64)\n",
    "       #     current_input = np.insert(h, 0, 1.0).astype(np.float64).reshape(-1,1)\n",
    "            if current_input.shape != (self.whole_network_size[i+1] + 1,x.shape[0]):\n",
    "                logging.warning(\"current_input shape is \" + str(current_input.shape) + \" when it is supposed to be \" + str((self.whole_network_size[i+1] + 1,x.shape[0])))\n",
    "            inputs.append(current_input)\n",
    "        #use softmax for final layer to compute the output, current input is now the input of the final hidden layer\n",
    "        z = np.dot(self.weights[-1], current_input)\n",
    "        if z.shape != (self.whole_network_size[-1], x.shape[0]):\n",
    "            logging.warning(\"final z shape is \" + str(z.shape))\n",
    "            \n",
    "        #output has the shape N x M, where N is the features and M is the number of training example\n",
    "            \n",
    "        output = self.softmax(z)  \n",
    "        \n",
    "        \n",
    "        return (inputs, output)\n",
    "    def backward_propagation(self, inputs = list, output = np.array, y_label = np.array):\n",
    "        # deltas is a list of vectors with shape identical to the corresponding neuron node of each layers \n",
    "        # gradients is a list of matrices of the gradient of cost functions. such list and matrices should have the \\\n",
    "        # same shape as the list weights\n",
    "        # back prop is performed on 1 training example each time (stochastic)\n",
    "        global gradients\n",
    "        global deltas\n",
    "        deltas = []\n",
    "        gradients = []\n",
    "\n",
    "        #compute the delta value of the last layer (output layer) first then propagate backward with loop\n",
    "        delta = (output - y_label).reshape(-1,1)\n",
    "#         print (output)\n",
    "#         print (y_label)\n",
    "        if delta.shape != output.shape:\n",
    "            logging.warning(\"initial delta shape is \" + str(delta.shape))            \n",
    "        deltas.append(delta)\n",
    "        \n",
    "        \n",
    "#         print (deltas)\n",
    "        # working backward from the last hidden layer, the gradient is computed and stored.\n",
    "        # the delta value of such layer is then computed and stored\n",
    "        # for the last layer (input), there's no regularisation term\n",
    "        for i in range(len(self.whole_network_size)-2, -1, -1):\n",
    "      #      print (i)\n",
    "            if i == 0:\n",
    "                lamb = 0\n",
    "            else:\n",
    "                lamb = self.lamb\n",
    "                \n",
    "            # if gradient return a single number, it's wrong. Should be the same shape as weight.\n",
    "            if i == len(self.whole_network_size)-2:\n",
    "                if delta.shape != (self.weights[i].shape[0], 1):\n",
    "                    logging.warning(str(i) +\" delta shape is\" + str(delta.shape) + \" when it is supposed to be \" + str((self.weights[i].shape[0], 1)))\n",
    "            else:\n",
    "                if delta.shape != (self.weights[i].shape[0], 1):\n",
    "                    logging.warning(str(i) +\" delta shape is\" + str(delta.shape) + \" when it is supposed to be \" + str((self.weights[i].shape[0], 1)))\n",
    "                \n",
    "            if inputs[i].T.shape != (1, self.weights[i].shape[1]):\n",
    "                logging.warning(\"input shape is\" + str(inputs[i].T.shape))\n",
    "                \n",
    "            gradient = np.dot(delta, inputs[i].T)  + (self.lamb/ self.M) * self.weights[i]\n",
    "#             if gradient.shape != (self.weights[i].shape:\n",
    "#                 logging.warning(\"gradient shape is\" + str(gradient.shape)  when it should be )\n",
    "                \n",
    "            gradients.insert(0, gradient)\n",
    "            \n",
    "            delta = np.dot(self.weights[i].T, delta) * inputs[i] * (1 - inputs[i])\n",
    "            delta = np.delete(delta, 0, axis = 0)\n",
    "            deltas.insert(0, delta)\n",
    "#            print (deltas)\n",
    "        # check if shape of gradients is the same as weights\n",
    "        for i in range(len(gradients)):\n",
    "            if gradients[i].shape != self.weights[i].shape:\n",
    "                logging.warning(\"gradient shape is\" + str(gradient.shape))\n",
    "        return gradients\n",
    "\n",
    "    def gradient_descent(self, gradients = list):\n",
    "        #gradients should have the exact same structure as weights (list of matrices)\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] = self.weights[i] - self.alpha * gradients[i] # regularisation term already taken into account during back prop\n",
    "    def label_convert(self, y):\n",
    "        converted_output = []\n",
    "        for label in y:\n",
    "            output_vector = np.zeros(np.unique(y).size)\n",
    "            index_num = self.unique_class.index(label)\n",
    "            output_vector[index_num] = 1\n",
    "            converted_output.append(output_vector)\n",
    "        return np.array(converted_output)    \n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        global inputs\n",
    "        losses = {}\n",
    "        step = 0\n",
    "        np.random.shuffle(self.dataset)\n",
    "        X_shuffled, y_shuffled = np.hsplit(self.dataset, [self.dataset.shape[1]- 1,])\n",
    "        y_shuffled = self.label_convert(y_shuffled)  \n",
    "        for index in itertools.cycle(range(X_shuffled.shape[0])):\n",
    "            x = X_shuffled[index].reshape(1,-1)\n",
    "            inputs, output = self.forward_propagation(x)\n",
    "            \n",
    "            #compute the loss function here before gradient descent for every 10 iteration\n",
    "            if step %10 == 0:\n",
    "                _, outputs = self.forward_propagation(X_shuffled)\n",
    "                loss = (-1/y_shuffled.shape[0]) * np.sum(y_shuffled * np.log(outputs.T))\n",
    "                losses[step] = loss\n",
    "            \n",
    "            #update the weight\n",
    "            gradients = self.backward_propagation (inputs = inputs, output = output, y_label = y_shuffled[index].reshape(-1,1))\n",
    "            self.gradient_descent(gradients = gradients)    \n",
    "            step += 1\n",
    "            \n",
    "            #if number of iteration specified met, stop optimising.\n",
    "            if step > self.num_iter:\n",
    "                break\n",
    "        \n",
    "        return (losses)\n",
    "    def predict(self, X_test = np.array):\n",
    "        global outputs\n",
    "        global class_output\n",
    "        class_output = []\n",
    "        #convert the vector into the corresponding class with the highest score,\n",
    "        inputs, outputs = self.forward_propagation(X_test)\n",
    "        outputs[outputs == np.max(outputs, axis = 0)] = 1\n",
    "        outputs[outputs != np.max(outputs, axis = 0)] = 0\n",
    "        for output in outputs.T.tolist():\n",
    "            index_num = output.index(1)\n",
    "            clas = self.unique_class[index_num]\n",
    "            class_output.append(clas)\n",
    "\n",
    "        return np.array(class_output)\n",
    "\n",
    "    def score(self, X_test = np.array, y_test = np.array):\n",
    "        #compare the output to the actual label and work out the accuracy ratio.\n",
    "        predicted_outputs = self.predict(X_test)\n",
    "        accu = (predicted_outputs == y_test).sum()/len(predicted_outputs)\n",
    "        return accu          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96999999999999997"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "A = np.random.multivariate_normal([7, 6], [[2, 1],[1, 2]], 1000)\n",
    "B = np.random.multivariate_normal([3, 0], [[2, 1],[1, 2]], 1000)\n",
    "C = np.random.multivariate_normal([0, 4], [[2, 1],[1, 2]], 1000)\n",
    "X = np.concatenate((A, B, C))\n",
    "y = np.concatenate((np.array([\"A\" for i in range(A.shape[0])]), \n",
    "                    np.array([\"B\" for i in range(B.shape[0])]), \n",
    "                    np.array([\"C\" for i in range(C.shape[0])])))\n",
    "cm = y.copy()\n",
    "cm[cm == \"A\"] = 1\n",
    "cm[cm == \"B\"] =2\n",
    "cm[cm ==\"C\"] =3\n",
    "plt.figure()\n",
    "plt.scatter(np.array([i[0] for i in X]), np.array([i[1] for i in X]), c = cm, alpha = 0.7)\n",
    "#plt.scatter(*np.hsplit(X, 2), c = y)\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.show()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C = Neural_network(X_train, y_train, hidden_layer_size = (100,), alpha = 0.1, num_iter = 200, lamb = 0)\n",
    "losses = C.fit()\n",
    "C.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98333333333333328"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes = (100,), activation = \"logistic\", solver = \"sgd\", max_iter=100,alpha=0.01, shuffle = True, learning_rate = \"constant\")\n",
    "mlp = mlp.fit(X_train,y_train)\n",
    "mlp.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20f7da3e5c0>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIyUFERFppKQgIiKNlBRERKTR/we13by+ysjj2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20f01269cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"loss vs number of iteration\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"number of iteration\")\n",
    "plt.grid()\n",
    "plt.plot(list(losses.keys()), list(losses.values()), marker = \"o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([i for i in range(15)]).reshape(3,5) , np.array([2 for i in range(15)]).reshape(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
